<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Emotion Detector</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, sans-serif;
            background: #000;
            color: white;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 20px;
            overflow-x: hidden;
        }
        
        .container {
            width: 100%;
            max-width: 600px;
            text-align: center;
        }
        
        h1 {
            font-size: 28px;
            margin-bottom: 10px;
            color: #4cc9f0;
            text-shadow: 0 0 10px rgba(76, 201, 240, 0.5);
        }
        
        .subtitle {
            color: #888;
            margin-bottom: 30px;
            font-size: 16px;
        }
        
        .video-container {
            position: relative;
            width: 100%;
            height: 400px;
            background: #111;
            border-radius: 20px;
            overflow: hidden;
            margin-bottom: 20px;
            border: 2px solid #333;
        }
        
        video, canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        
        .emotions-container {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            justify-content: center;
            margin: 20px 0;
        }
        
        .emotion-box {
            background: rgba(255, 255, 255, 0.05);
            padding: 15px;
            border-radius: 15px;
            min-width: 150px;
            border: 1px solid #333;
            flex: 1;
        }
        
        .face-label {
            font-size: 14px;
            color: #4cc9f0;
            margin-bottom: 5px;
            font-weight: bold;
        }
        
        .emotion-label {
            font-size: 12px;
            color: #888;
            margin-bottom: 5px;
        }
        
        .emotion-text {
            font-size: 24px;
            font-weight: bold;
            min-height: 36px;
            color: #4cc9f0;
        }
        
        .confidence {
            font-size: 12px;
            color: #888;
            margin-top: 5px;
        }
        
        .controls {
            display: flex;
            gap: 10px;
            margin: 20px 0;
            flex-wrap: wrap;
            justify-content: center;
        }
        
        button {
            background: #4cc9f0;
            color: black;
            border: none;
            padding: 15px 25px;
            border-radius: 10px;
            font-size: 16px;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s;
            min-width: 150px;
        }
        
        button:hover {
            background: #3ab8df;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(76, 201, 240, 0.4);
        }
        
        button:disabled {
            background: #555;
            color: #888;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        
        #stopBtn {
            background: #f04c4c;
        }
        
        #stopBtn:hover {
            background: #df3a3a;
        }
        
        .status {
            padding: 15px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 10px;
            margin: 20px 0;
            font-size: 14px;
            color: #4cc9f0;
            min-height: 50px;
            display: flex;
            align-items: center;
            justify-content: center;
            border: 1px solid #333;
        }
        
        .loading {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            text-align: center;
            z-index: 100;
            background: rgba(0, 0, 0, 0.9);
            padding: 30px;
            border-radius: 15px;
            border: 2px solid #4cc9f0;
        }
        
        .instructions {
            text-align: left;
            background: rgba(255, 255, 255, 0.05);
            padding: 15px;
            border-radius: 10px;
            margin-top: 20px;
            font-size: 14px;
            color: #888;
            border: 1px solid #333;
        }
        
        .instructions h3 {
            color: #4cc9f0;
            margin-bottom: 10px;
            text-align: center;
        }
        
        .instructions ul {
            list-style: none;
            padding-left: 10px;
        }
        
        .instructions li {
            margin: 8px 0;
            padding-left: 20px;
            position: relative;
        }
        
        .instructions li:before {
            content: "‚Ä¢";
            color: #4cc9f0;
            position: absolute;
            left: 0;
        }
        
        @media (max-width: 600px) {
            .video-container {
                height: 300px;
            }
            
            .emotion-box {
                min-width: 120px;
                padding: 10px;
            }
            
            .emotion-text {
                font-size: 20px;
            }
            
            button {
                min-width: 130px;
                padding: 12px 20px;
                font-size: 14px;
            }
            
            h1 {
                font-size: 24px;
            }
        }
        
        .hidden {
            display: none !important;
        }
        
        .emoji {
            font-size: 28px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé≠ Face Emotion Detector</h1>
        <p class="subtitle">Real-time facial expression analysis for multiple faces</p>
        
        <div class="video-container">
            <video id="video" autoplay playsinline></video>
            <canvas id="canvas"></canvas>
            <div class="loading" id="loading">
                <p>üîÑ Loading AI Model...</p>
                <div class="status" id="statusText">Initializing...</div>
            </div>
        </div>
        
        <div class="status" id="status">Ready to start detection</div>
        
        <div class="emotions-container" id="emotionsContainer">
            <!-- Dynamic emotion boxes will be inserted here -->
        </div>
        
        <div class="controls">
            <button id="startBtn" onclick="startCamera()" disabled>Loading...</button>
            <button id="stopBtn" onclick="stopCamera()" class="hidden">Stop Camera</button>
        </div>
        
        <div class="instructions">
            <h3>How to Use:</h3>
            <ul>
                <li>Allow camera access when prompted</li>
                <li>Ensure good lighting on faces</li>
                <li>Can detect up to 4 faces simultaneously</li>
                <li>Try these expressions:</li>
                <li>üòä Smile for HAPPY</li>
                <li>üòÆ Open mouth wide for SURPRISED</li>
                <li>üòî Frown for SAD</li>
                <li>üò† Furrow brows for ANGRY</li>
                <li>üòê Neutral expression</li>
            </ul>
        </div>
    </div>

    <script>
        // DOM Elements
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusElement = document.getElementById('status');
        const statusText = document.getElementById('statusText');
        const loadingElement = document.getElementById('loading');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const emotionsContainer = document.getElementById('emotionsContainer');
        
        // State variables
        let faceMesh = null;
        let cameraRunning = false;
        let isModelLoaded = false;
        let lastDetectionTime = 0;
        const DETECTION_INTERVAL = 100; // ms
        
        // Emotion emoji mapping
        const EMOTION_EMOJIS = {
            'HAPPY': 'üòä',
            'SAD': 'üòî',
            'ANGRY': 'üò†',
            'SURPRISED': 'üòÆ',
            'FEAR': 'üò®',
            'DISGUST': 'ü§¢',
            'NEUTRAL': 'üòê',
            'CONFUSED': 'ü§î',
            'SLEEPY': 'üò¥',
            'UNKNOWN': 'ü§∑'
        };
        
        // Initialize when page loads
        window.addEventListener('DOMContentLoaded', () => {
            statusText.textContent = 'Loading MediaPipe scripts...';
            loadMediaPipe();
        });
        
        // Load MediaPipe from CDN
        function loadMediaPipe() {
            // Create script elements
            const faceMeshScript = document.createElement('script');
            faceMeshScript.src = 'https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js';
            faceMeshScript.crossOrigin = 'anonymous';
            
            const cameraUtilsScript = document.createElement('script');
            cameraUtilsScript.src = 'https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js';
            cameraUtilsScript.crossOrigin = 'anonymous';
            
            const drawingUtilsScript = document.createElement('script');
            drawingUtilsScript.src = 'https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js';
            drawingUtilsScript.crossOrigin = 'anonymous';
            
            // Load scripts in sequence
            faceMeshScript.onload = () => {
                statusText.textContent = 'Loaded Face Mesh...';
                document.head.appendChild(cameraUtilsScript);
                
                cameraUtilsScript.onload = () => {
                    statusText.textContent = 'Loaded Camera Utils...';
                    document.head.appendChild(drawingUtilsScript);
                    
                    drawingUtilsScript.onload = () => {
                        statusText.textContent = 'Initializing FaceMesh...';
                        setTimeout(() => {
                            initFaceMesh();
                        }, 500);
                    };
                    
                    drawingUtilsScript.onerror = () => {
                        showError('Failed to load drawing utils');
                    };
                };
                
                cameraUtilsScript.onerror = () => {
                    showError('Failed to load camera utils');
                };
            };
            
            faceMeshScript.onerror = () => {
                showError('Failed to load FaceMesh. Check internet connection.');
            };
            
            document.head.appendChild(faceMeshScript);
        }
        
        function showError(message) {
            statusText.textContent = `Error: ${message}`;
            statusText.style.color = '#f04c4c';
            startBtn.textContent = 'Error - Refresh Page';
            startBtn.disabled = true;
        }
        
        function initFaceMesh() {
            try {
                if (typeof FaceMesh === 'undefined') {
                    throw new Error('FaceMesh not loaded');
                }
                
                faceMesh = new FaceMesh({
                    locateFile: (file) => {
                        return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
                    }
                });
                
                faceMesh.setOptions({
                    maxNumFaces: 4, // CHANGED: Now supports up to 4 faces
                    refineLandmarks: true,
                    minDetectionConfidence: 0.5,
                    minTrackingConfidence: 0.5,
                    selfieMode: true
                });
                
                faceMesh.onResults(onResults);
                
                // Update UI
                isModelLoaded = true;
                startBtn.disabled = false;
                startBtn.textContent = 'üé• Start Camera';
                statusText.textContent = '‚úÖ AI Model Loaded!';
                statusElement.textContent = 'Ready to detect emotions from multiple faces';
                
                // Hide loading after a delay
                setTimeout(() => {
                    loadingElement.classList.add('hidden');
                }, 1000);
                
                // Create initial emotion boxes
                updateEmotionBoxes(0);
                
            } catch (error) {
                showError(error.message);
            }
        }
        
        function onResults(results) {
            if (!cameraRunning) return;
            
            const currentTime = Date.now();
            if (currentTime - lastDetectionTime < DETECTION_INTERVAL) {
                // Skip frame to maintain performance
                requestAnimationFrame(() => {
                    faceMesh.send({ image: video });
                });
                return;
            }
            lastDetectionTime = currentTime;
            
            // Set canvas dimensions
            canvas.width = video.videoWidth || 640;
            canvas.height = video.videoHeight || 480;
            
            // Clear canvas
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            // Draw video (mirrored)
            ctx.save();
            ctx.translate(canvas.width, 0);
            ctx.scale(-1, 1);
            ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);
            ctx.restore();
            
            // Check for faces
            if (!results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) {
                statusElement.textContent = 'No faces detected - make sure faces are visible';
                updateEmotionBoxes(0);
                return;
            }
            
            const faces = results.multiFaceLandmarks;
            const faceDetections = [];
            
            ctx.save();
            ctx.translate(canvas.width, 0);
            ctx.scale(-1, 1);
            
            // Process each face
            for (let i = 0; i < faces.length; i++) {
                const landmarks = faces[i];
                
                // Get key facial points with improved indices
                const leftEye = [landmarks[33], landmarks[7], landmarks[163], landmarks[144]];
                const rightEye = [landmarks[362], landmarks[381], landmarks[380], landmarks[374]];
                const mouth = [landmarks[13], landmarks[14], landmarks[78], landmarks[308]];
                const leftEyebrow = [landmarks[70], landmarks[46]];
                const rightEyebrow = [landmarks[300], landmarks[276]];
                const nose = landmarks[1];
                const jaw = landmarks[152];
                
                // Calculate improved metrics
                const leftEyeAspect = calculateEyeAspectRatio(leftEye);
                const rightEyeAspect = calculateEyeAspectRatio(rightEye);
                const eyeOpenness = (leftEyeAspect + rightEyeAspect) / 2;
                
                const mouthOpenness = calculateMouthOpenness(
                    landmarks[13], landmarks[14], landmarks[78], landmarks[308]
                );
                
                const mouthWidth = getDistance(landmarks[78], landmarks[308]);
                const eyebrowTilt = calculateEyebrowTilt(leftEyebrow, rightEyebrow, nose);
                
                // Detect emotion with improved algorithm
                const { emotion, confidence } = detectEmotion(
                    eyeOpenness,
                    mouthOpenness,
                    mouthWidth,
                    eyebrowTilt,
                    landmarks
                );
                
                faceDetections.push({
                    index: i,
                    emotion: emotion,
                    confidence: confidence,
                    landmarks: landmarks
                });
                
                // Draw face boundary and label
                drawFaceInfo(ctx, landmarks, i, emotion, confidence);
            }
            
            ctx.restore();
            
            // Update UI with all face emotions
            updateEmotionBoxes(faceDetections.length, faceDetections);
            statusElement.textContent = `Detecting ${faceDetections.length} face${faceDetections.length !== 1 ? 's' : ''}`;
            
            // Continue processing
            if (cameraRunning) {
                requestAnimationFrame(() => {
                    faceMesh.send({ image: video });
                });
            }
        }
        
        function calculateEyeAspectRatio(eyePoints) {
            // Vertical distances
            const vertical1 = getDistance(eyePoints[1], eyePoints[5]);
            const vertical2 = getDistance(eyePoints[2], eyePoints[4]);
            
            // Horizontal distance
            const horizontal = getDistance(eyePoints[0], eyePoints[3]);
            
            // Eye aspect ratio
            return (vertical1 + vertical2) / (2.0 * horizontal);
        }
        
        function calculateMouthOpenness(top, bottom, left, right) {
            const vertical = getDistance(top, bottom);
            const horizontal = getDistance(left, right);
            return vertical / horizontal;
        }
        
        function calculateEyebrowTilt(leftEyebrow, rightEyebrow, nose) {
            const leftDist = getDistance(leftEyebrow[0], nose);
            const rightDist = getDistance(rightEyebrow[0], nose);
            return (rightDist - leftDist) / (leftDist + rightDist);
        }
        
        function detectEmotion(eyeOpenness, mouthOpenness, mouthWidth, eyebrowTilt, landmarks) {
            // Improved emotion detection logic
            let emotion = 'NEUTRAL';
            let confidence = 50;
            
            // Check for happiness (smile)
            if (mouthOpenness > 0.15 && mouthWidth > 0.12 && eyebrowTilt > -0.1) {
                emotion = 'HAPPY';
                confidence = Math.min(95, 60 + (mouthOpenness * 200));
            }
            // Check for surprise (wide eyes and mouth)
            else if (eyeOpenness > 0.35 && mouthOpenness > 0.25) {
                emotion = 'SURPRISED';
                confidence = Math.min(95, 65 + ((eyeOpenness + mouthOpenness) * 150));
            }
            // Check for sadness (drooping features)
            else if (mouthOpenness < 0.08 && mouthWidth > 0.1 && eyebrowTilt > 0.15) {
                emotion = 'SAD';
                confidence = Math.min(90, 60 + Math.abs(eyebrowTilt) * 100);
            }
            // Check for anger (furrowed brows, tight mouth)
            else if (eyeOpenness < 0.2 && mouthOpenness < 0.06 && eyebrowTilt < -0.1) {
                emotion = 'ANGRY';
                confidence = Math.min(90, 65 + Math.abs(eyebrowTilt) * 120);
            }
            // Check for fear (wide eyes, tight mouth)
            else if (eyeOpenness > 0.35 && mouthOpenness < 0.06) {
                emotion = 'FEAR';
                confidence = Math.min(85, 60 + (eyeOpenness * 150));
            }
            // Check for disgust (nose wrinkle, mouth asymmetry)
            else if (mouthOpenness > 0.1 && mouthOpenness < 0.15 && Math.abs(eyebrowTilt) > 0.2) {
                emotion = 'DISGUST';
                confidence = 70;
            }
            // Check for sleepy (eyes closed)
            else if (eyeOpenness < 0.15) {
                emotion = 'SLEEPY';
                confidence = Math.min(95, 70 + ((0.15 - eyeOpenness) * 300));
            }
            // Check for confusion (asymmetric features)
            else if (Math.abs(eyebrowTilt) > 0.25) {
                emotion = 'CONFUSED';
                confidence = Math.min(80, 60 + Math.abs(eyebrowTilt) * 80);
            }
            
            // Normalize confidence
            confidence = Math.round(Math.min(99, Math.max(20, confidence)));
            
            return { emotion, confidence };
        }
        
        function drawFaceInfo(ctx, landmarks, faceIndex, emotion, confidence) {
            // Calculate face bounding box
            let minX = Infinity, minY = Infinity, maxX = 0, maxY = 0;
            
            for (const point of landmarks) {
                minX = Math.min(minX, point.x * canvas.width);
                minY = Math.min(minY, point.y * canvas.height);
                maxX = Math.max(maxX, point.x * canvas.width);
                maxY = Math.max(maxY, point.y * canvas.height);
            }
            
            const width = maxX - minX;
            const height = maxY - minY;
            
            // Draw face bounding box
            ctx.strokeStyle = faceIndex === 0 ? '#4cc9f0' : 
                             faceIndex === 1 ? '#f04c4c' : 
                             faceIndex === 2 ? '#4cf04c' : '#f0f04c';
            ctx.lineWidth = 3;
            ctx.strokeRect(minX - 10, minY - 10, width + 20, height + 20);
            
            // Draw label background
            ctx.fillStyle = 'rgba(0, 0, 0, 0.7)';
            ctx.fillRect(minX - 10, minY - 40, 150, 30);
            
            // Draw label text
            ctx.fillStyle = 'white';
            ctx.font = '14px Arial';
            ctx.fillText(`Face ${faceIndex + 1}: ${emotion} ${EMOTION_EMOJIS[emotion]}`, minX - 5, minY - 20);
            
            // Draw confidence
            ctx.font = '12px Arial';
            ctx.fillStyle = '#888';
            ctx.fillText(`${confidence}%`, minX - 5, minY - 5);
            
            // Draw key points
            ctx.fillStyle = faceIndex === 0 ? '#4cc9f0' : 
                           faceIndex === 1 ? '#f04c4c' : 
                           faceIndex === 2 ? '#4cf04c' : '#f0f04c';
            const keyPoints = [33, 133, 362, 263, 13, 14, 78, 308, 1, 152];
            for (const index of keyPoints) {
                const point = landmarks[index];
                ctx.beginPath();
                ctx.arc(point.x * canvas.width, point.y * canvas.height, 3, 0, 2 * Math.PI);
                ctx.fill();
            }
        }
        
        function getDistance(p1, p2) {
            const dx = p2.x - p1.x;
            const dy = p2.y - p1.y;
            return Math.sqrt(dx * dx + dy * dy);
        }
        
        function updateEmotionBoxes(numFaces, faceDetections = []) {
            emotionsContainer.innerHTML = '';
            
            if (numFaces === 0) {
                const box = document.createElement('div');
                box.className = 'emotion-box';
                box.innerHTML = `
                    <div class="face-label">No Faces</div>
                    <div class="emotion-label">EMOTION</div>
                    <div class="emotion-text">--</div>
                    <div class="confidence">Waiting for faces...</div>
                `;
                emotionsContainer.appendChild(box);
                return;
            }
            
            for (let i = 0; i < numFaces; i++) {
                const box = document.createElement('div');
                box.className = 'emotion-box';
                
                let emotion = '--';
                let confidence = '--';
                let emoji = '';
                
                if (i < faceDetections.length) {
                    const detection = faceDetections[i];
                    emotion = detection.emotion;
                    confidence = detection.confidence + '%';
                    emoji = EMOTION_EMOJIS[emotion] || EMOTION_EMOJIS.UNKNOWN;
                }
                
                box.innerHTML = `
                    <div class="face-label">Face ${i + 1}</div>
                    <div class="emotion-label">EMOTION</div>
                    <div class="emotion-text">${emoji} ${emotion}</div>
                    <div class="confidence">Confidence: ${confidence}</div>
                `;
                emotionsContainer.appendChild(box);
            }
        }
        
        async function startCamera() {
            if (!isModelLoaded) {
                alert('AI model is still loading. Please wait...');
                return;
            }
            
            try {
                statusElement.textContent = 'Requesting camera access...';
                startBtn.disabled = true;
                
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: 'user',
                        width: { ideal: 640 },
                        height: { ideal: 480 }
                    },
                    audio: false
                });
                
                video.srcObject = stream;
                cameraRunning = true;
                
                video.onloadedmetadata = () => {
                    // Update UI
                    startBtn.classList.add('hidden');
                    stopBtn.classList.remove('hidden');
                    statusElement.textContent = 'Camera ready! Detecting emotions...';
                    
                    // Start detection
                    faceMesh.send({ image: video });
                };
                
            } catch (error) {
                console.error('Camera error:', error);
                let errorMessage = 'Camera access denied';
                
                if (error.name === 'NotFoundError') {
                    errorMessage = 'No camera found';
                } else if (error.name === 'NotAllowedError') {
                    errorMessage = 'Camera permission denied';
                } else if (error.name === 'NotReadableError') {
                    errorMessage = 'Camera is in use by another app';
                }
                
                statusElement.textContent = `Error: ${errorMessage}`;
                startBtn.disabled = false;
                startBtn.textContent = 'Try Again';
                updateEmotionBoxes(0);
            }
        }
        
        function stopCamera() {
            cameraRunning = false;
            
            if (video.srcObject) {
                const tracks = video.srcObject.getTracks();
                tracks.forEach(track => track.stop());
                video.srcObject = null;
            }
            
            // Clear canvas
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            // Update UI
            startBtn.classList.remove('hidden');
            startBtn.disabled = false;
            stopBtn.classList.add('hidden');
            statusElement.textContent = 'Ready to start';
            updateEmotionBoxes(0);
        }
        
        // Handle page visibility
        document.addEventListener('visibilitychange', () => {
            if (document.hidden && cameraRunning) {
                statusElement.textContent = 'Tab inactive - detection paused';
            } else if (cameraRunning) {
                statusElement.textContent = 'Resuming detection...';
            }
        });
    </script>
</body>
</html>
